# -*- coding: utf-8 -*-
"""fake_news_detection.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1hWrS1xJcIa2b1mafm8M_d0VfQcsmPMg7
"""

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from wordcloud import WordCloud
import gdown

file_ids = {
    'train': '1v3p0txA9OSYt03lECxo0N6IeuxjT4ADJ',
    'test': '1_Qh9dmci6HeBj9DJNMJ-Bfvh87F_N5AK',
    'submission': '12JvvzdnBa-VMiSzQ8G9hUBUudbxbmh4Q'
}

# Download files
for name, file_id in file_ids.items():
    gdown.download(f'https://drive.google.com/uc?id={file_id}', f'{name}.csv', quiet=False)

# Read CSV files into pandas DataFrames
train = pd.read_csv('train.csv')
test = pd.read_csv('test.csv')
submission = pd.read_csv('submission.csv')

train.head()

train.info()

train.isnull().sum()

train.shape

train['keyword'].fillna('NoKeyword', inplace=True)
train['location'].fillna('NoLocation', inplace=True)

train.isnull().sum()

import seaborn as sns
import matplotlib.pyplot as plt

sns.countplot(x='target', data=train)
plt.title('Distribution of Real (1) vs Fake (0) Tweets')
plt.show()

train['keyword'].value_counts().head(10)

train['keyword'].value_counts().head(10).plot(kind='bar')
plt.title('Top 10 Keywords')
plt.xlabel('Keyword')
plt.ylabel('Frequency')
plt.show()

train['location'].value_counts().head(10)

from wordcloud import WordCloud

all_words = ' '.join(train['text'])

wordcloud = WordCloud(width=800, height=400, background_color='white').generate(all_words)

plt.figure(figsize=(12, 6))
plt.imshow(wordcloud, interpolation='bilinear')
plt.axis('off')
plt.title('WordCloud of All Tweets')
plt.show()

import re

def clean_text(text):
    text = re.sub(r'[^a-zA-Z]', ' ', text)  # keep letters, remove others
    text = text.lower()                     # convert to lowercase
    text = text.split()                     # tokenize words
    return text

import nltk
nltk.download('stopwords')
from nltk.corpus import stopwords

stop_words = set(stopwords.words('english'))

def clean_text(text):
    text = re.sub(r'[^a-zA-Z]', ' ', text)
    text = text.lower()
    words = text.split()
    words = [word for word in words if word not in stop_words]
    return words

from nltk.stem import PorterStemmer
import spacy

stemmer = PorterStemmer()
nlp = spacy.load('en_core_web_sm')

def stem_words(words):
    return [stemmer.stem(word) for word in words]

def lemmatize_words(words):
    doc = nlp(' '.join(words))
    return [token.lemma_ for token in doc]

def preprocess_text(text):
    text = re.sub(r'[^a-zA-Z]', ' ', text)
    text = text.lower()
    words = text.split()
    words = [word for word in words if word not in stop_words]
    words = [stemmer.stem(word) for word in words]
    # words = lemmatize_words(words)  # if you prefer lemmatization
    return ' '.join(words)

# Apply to your dataset
train['clean_text'] = train['text'].apply(preprocess_text)

all_words = ' '.join(train['clean_text'])

wordcloud = WordCloud(width=800, height=400, background_color='white').generate(all_words)

plt.figure(figsize=(12, 6))
plt.imshow(wordcloud, interpolation='bilinear')
plt.axis('off')
plt.title('WordCloud of Cleaned Tweets')
plt.show()

from sklearn.feature_extraction.text import CountVectorizer

# Initialize vectorizer
vectorizer = CountVectorizer(max_features=5000)  # Use top 5000 words

# Fit and transform on training data
X_bow = vectorizer.fit_transform(train['clean_text'])

# Check shape (rows = number of tweets, columns = number of unique words)
X_bow.shape

from sklearn.feature_extraction.text import TfidfVectorizer

# Initialize vectorizer
tfidf_vectorizer = TfidfVectorizer(max_features=5000)

# Fit and transform on training data
X_tfidf = tfidf_vectorizer.fit_transform(train['clean_text'])

# Check shape
X_tfidf.shape

from transformers import BertTokenizer, BertModel
import torch

# Load pre-trained BERT model and tokenizer
tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')
model = BertModel.from_pretrained('bert-base-uncased')

# Encode a sample tweet
inputs = tokenizer("earthquake in city", return_tensors='pt')

# Get BERT embeddings
with torch.no_grad():
    outputs = model(**inputs)

# Get the vector for [CLS] token (sentence representation)
cls_embedding = outputs.last_hidden_state[:, 0, :]

print(cls_embedding.shape)

from sklearn.model_selection import train_test_split

# Use TF-IDF features (you can use X_bow too)
X = X_tfidf
y = train['target']

# Split data (80% train, 20% test)
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Check shape
X_train.shape, X_test.shape

from sklearn.linear_model import LogisticRegression
from sklearn.metrics import classification_report, confusion_matrix, accuracy_score

# Initialize and train model
lr_model = LogisticRegression(max_iter=1000)
lr_model.fit(X_train, y_train)

# Predict on test set
y_pred_lr = lr_model.predict(X_test)

# Evaluate
print("Logistic Regression Results:")
print(confusion_matrix(y_test, y_pred_lr))
print(classification_report(y_test, y_pred_lr))
print("Accuracy:", accuracy_score(y_test, y_pred_lr))

from sklearn.svm import LinearSVC

# Initialize and train model
svm_model = LinearSVC()
svm_model.fit(X_train, y_train)

# Predict
y_pred_svm = svm_model.predict(X_test)

# Evaluate
print("SVM Results:")
print(confusion_matrix(y_test, y_pred_svm))
print(classification_report(y_test, y_pred_svm))
print("Accuracy:", accuracy_score(y_test, y_pred_svm))

from sklearn.naive_bayes import MultinomialNB

# Initialize and train model
nb_model = MultinomialNB()
nb_model.fit(X_train, y_train)

# Predict
y_pred_nb = nb_model.predict(X_test)

# Evaluate
print("Naive Bayes Results:")
print(confusion_matrix(y_test, y_pred_nb))
print(classification_report(y_test, y_pred_nb))
print("Accuracy:", accuracy_score(y_test, y_pred_nb))

#checkong all
from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix

def evaluate_model(y_test, y_pred):
    acc = accuracy_score(y_test, y_pred)
    prec = precision_score(y_test, y_pred)
    rec = recall_score(y_test, y_pred)
    f1 = f1_score(y_test, y_pred)
    cm = confusion_matrix(y_test, y_pred)
    return acc, prec, rec, f1, cm


# Logistic Regression
acc_lr, prec_lr, rec_lr, f1_lr, cm_lr = evaluate_model(y_test, y_pred_lr)

# SVM
acc_svm, prec_svm, rec_svm, f1_svm, cm_svm = evaluate_model(y_test, y_pred_svm)

# Naive Bayes
acc_nb, prec_nb, rec_nb, f1_nb, cm_nb = evaluate_model(y_test, y_pred_nb)


import pandas as pd

results = pd.DataFrame({
    'Model': ['Logistic Regression', 'SVM (LinearSVC)', 'Naive Bayes'],
    'Accuracy': [acc_lr, acc_svm, acc_nb],
    'Precision': [prec_lr, prec_svm, prec_nb],
    'Recall': [rec_lr, rec_svm, rec_nb],
    'F1-Score': [f1_lr, f1_svm, f1_nb]
})

# Display nicely
results = results.round(4)
results



import seaborn as sns
import matplotlib.pyplot as plt

def plot_confusion(cm, model_name):
    plt.figure(figsize=(5, 4))
    sns.heatmap(cm, annot=True, fmt="d", cmap="Blues")
    plt.title(f'Confusion Matrix - {model_name}')
    plt.xlabel('Predicted')
    plt.ylabel('Actual')
    plt.show()

# Plot for each model
plot_confusion(cm_lr, 'Logistic Regression')
plot_confusion(cm_svm, 'SVM (LinearSVC)')
plot_confusion(cm_nb, 'Naive Bayes')

from sklearn.model_selection import GridSearchCV
from sklearn.linear_model import LogisticRegression

param_grid_lr = {
    'C': [0.01, 0.1, 1, 10],
    'penalty': ['l2'],
    'solver': ['liblinear']  # required for L2 penalty
}

grid_lr = GridSearchCV(LogisticRegression(max_iter=1000), param_grid_lr, cv=5, scoring='f1', verbose=1, n_jobs=-1)
grid_lr.fit(X_train, y_train)

# Best estimator and parameters
print("Best Logistic Regression Params:", grid_lr.best_params_)

# Evaluate best model
y_pred_lr_tuned = grid_lr.predict(X_test)

from sklearn.svm import LinearSVC

param_grid_svm = {
    'C': [0.01, 0.1, 1, 10],
    'loss': ['hinge', 'squared_hinge']
}

grid_svm = GridSearchCV(LinearSVC(max_iter=10000), param_grid_svm, cv=5, scoring='f1', verbose=1, n_jobs=-1)
grid_svm.fit(X_train, y_train)

# Best estimator and parameters
print("Best SVM Params:", grid_svm.best_params_)

# Evaluate best model
y_pred_svm_tuned = grid_svm.predict(X_test)

def evaluate_model(y_test, y_pred):
    acc = accuracy_score(y_test, y_pred)
    prec = precision_score(y_test, y_pred)
    rec = recall_score(y_test, y_pred)
    f1 = f1_score(y_test, y_pred)
    return acc, prec, rec, f1

# Collect results
lr_metrics = evaluate_model(y_test, y_pred_lr_tuned)
svm_metrics = evaluate_model(y_test, y_pred_svm_tuned)

# Display comparison
tuned_results = pd.DataFrame({
    'Model': ['Logistic Regression (Tuned)', 'SVM (Tuned)'],
    'Accuracy': [lr_metrics[0], svm_metrics[0]],
    'Precision': [lr_metrics[1], svm_metrics[1]],
    'Recall': [lr_metrics[2], svm_metrics[2]],
    'F1-Score': [lr_metrics[3], svm_metrics[3]]
})

tuned_results.round(4)

from sklearn.metrics import roc_curve, auc

# Get probability scores
y_probs_lr = grid_lr.predict_proba(X_test)[:, 1]

# Compute ROC curve
fpr_lr, tpr_lr, _ = roc_curve(y_test, y_probs_lr)
roc_auc_lr = auc(fpr_lr, tpr_lr)

# Plot ROC
plt.figure(figsize=(7, 5))
plt.plot(fpr_lr, tpr_lr, color='darkorange', lw=2, label=f'Logistic Regression (AUC = {roc_auc_lr:.2f})')
plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')
plt.title('ROC Curve')
plt.xlabel('False Positive Rate')
plt.ylabel('True Positive Rate')
plt.legend(loc='lower right')
plt.grid(True)
plt.show()

from sklearn.metrics import precision_recall_curve, average_precision_score

precision_lr, recall_lr, _ = precision_recall_curve(y_test, y_probs_lr)
pr_auc_lr = average_precision_score(y_test, y_probs_lr)

plt.figure(figsize=(7, 5))
plt.plot(recall_lr, precision_lr, color='green', lw=2, label=f'Logistic Regression (AUC = {pr_auc_lr:.2f})')
plt.title('Precision-Recall Curve')
plt.xlabel('Recall')
plt.ylabel('Precision')
plt.legend(loc='lower left')
plt.grid(True)
plt.show()

# Find indexes where prediction != actual
misclassified_indices = np.where(y_test != y_pred_lr_tuned)[0]

# Display first 10 misclassified tweets
for i in misclassified_indices[:10]:
    print(f"Tweet: {train.iloc[y_test.index[i]]['text']}")
    print(f"Actual: {y_test.iloc[i]}, Predicted: {y_pred_lr_tuned[i]}\n")

import seaborn as sns

def plot_confusion(cm, model_name):
    plt.figure(figsize=(5, 4))
    sns.heatmap(cm, annot=True, fmt="d", cmap="YlGnBu")
    plt.title(f'Confusion Matrix - {model_name}')
    plt.xlabel('Predicted')
    plt.ylabel('Actual')
    plt.show()

# Confusion matrices
plot_confusion(confusion_matrix(y_test, y_pred_lr_tuned), 'Logistic Regression (Tuned)')
plot_confusion(confusion_matrix(y_test, y_pred_svm_tuned), 'SVM (Tuned)')



import joblib

# Save TF-IDF Vectorizer
joblib.dump(tfidf_vectorizer, 'tfidf_vectorizer.pkl')

# Save the trained Logistic Regression model
joblib.dump(grid_lr, 'fake_news_model.pkl')

import streamlit as st
import joblib
import re
import nltk
from nltk.corpus import stopwords
from nltk.stem import PorterStemmer

# Download stopwords if not downloaded
nltk.download('stopwords')

# Load model and vectorizer
model = joblib.load('fake_news_model.pkl')
vectorizer = joblib.load('tfidf_vectorizer.pkl')

# Preprocessing function
def preprocess_text(text):
    stop_words = set(stopwords.words('english'))
    stemmer = PorterStemmer()

    text = re.sub(r'[^a-zA-Z]', ' ', text)
    text = text.lower()
    words = text.split()
    words = [word for word in words if word not in stop_words]
    words = [stemmer.stem(word) for word in words]
    return ' '.join(words)

# Streamlit App
st.title("Twitter Disaster News Classifier üö®")

st.write("Enter a tweet text to check whether it is Real Disaster News or Not.")

# Input text box
user_input = st.text_area("Type a tweet here:", "")

if st.button("Predict"):
    if user_input.strip() == "":
        st.warning("Please enter some text.")
    else:
        # Preprocess input text
        clean_text = preprocess_text(user_input)

        # Vectorize
        text_vector = vectorizer.transform([clean_text])

        # Predict
        prediction = model.predict(text_vector)[0]

        # Show result
        if prediction == 1:
            st.success("‚úÖ This is **Real Disaster News**.")
        else:
            st.error("‚ùå This is **Not Disaster News**.")

